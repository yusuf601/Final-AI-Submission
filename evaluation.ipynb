{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2695280f-bec1-4bb9-8bfe-e68625785498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, \n",
    "    davies_bouldin_score, \n",
    "    calinski_harabasz_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff6379c-c756-4fe0-a3fa-cfd57120f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "OUTPUT_DIR = '/home/kali/AI/'\n",
    "\n",
    "df_features = pd.read_csv(OUTPUT_DIR + 'fcm_features_raw.csv', index_col=0)\n",
    "cluster_labels = np.load(OUTPUT_DIR + 'fcm_cluster_labels_c2.npy')\n",
    "membership_matrix = np.load(OUTPUT_DIR + 'fcm_membership_c2.npy')\n",
    "cluster_centers = np.load(OUTPUT_DIR + 'fcm_cluster_centers_c2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "607c506f-287b-42cb-b1d9-49d379b3e0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Loaded: 34 provinces, 50 features\n",
      "✓ Cluster distribution: C0=12, C1=22\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(df_features.values)\n",
    "\n",
    "provinces = df_features.index.tolist()\n",
    "\n",
    "print(f\"\\n✓ Loaded: {len(provinces)} provinces, {X_normalized.shape[1]} features\")\n",
    "print(f\"✓ Cluster distribution: C0={np.sum(cluster_labels==0)}, C1={np.sum(cluster_labels==1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dbb34f6-e055-478c-bd25-010312b4ccaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION 1: INTERNAL VALIDATION METRICS\n",
      "================================================================================\n",
      "\n",
      "✓ Silhouette Score: 0.2445\n",
      "  Interpretation: Weak structure (overlapping clusters)\n",
      "\n",
      "✓ Davies-Bouldin Index: 1.6812\n",
      "  Interpretation: Good (acceptable separation)\n",
      "\n",
      "✓ Calinski-Harabasz Score: 10.6152\n",
      "  Interpretation: Higher = better defined clusters\n",
      "\n",
      "✓ Fuzzy Partition Coefficient (FPC): 0.5608\n",
      "  Interpretation: Moderate fuzziness (acceptable)\n",
      "\n",
      "✓ Partition Entropy (PE): 0.6297\n",
      "  Max entropy (c=2): 0.6931\n",
      "  Interpretation: High entropy (fuzzy assignments)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION 1: INTERNAL VALIDATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1.1 Silhouette Score (-1 to 1, higher better)\n",
    "sil_score = silhouette_score(X_normalized, cluster_labels)\n",
    "print(f\"\\n✓ Silhouette Score: {sil_score:.4f}\")\n",
    "print(f\"  Interpretation: \", end='')\n",
    "if sil_score > 0.5:\n",
    "    print(\"Excellent separation\")\n",
    "elif sil_score > 0.25:\n",
    "    print(\"Good separation (reasonable structure)\")\n",
    "elif sil_score > 0:\n",
    "    print(\"Weak structure (overlapping clusters)\")\n",
    "else:\n",
    "    print(\"Poor clustering (misclassified points)\")\n",
    "\n",
    "# 1.2 Davies-Bouldin Index (0 to ∞, lower better)\n",
    "dbi_score = davies_bouldin_score(X_normalized, cluster_labels)\n",
    "print(f\"\\n✓ Davies-Bouldin Index: {dbi_score:.4f}\")\n",
    "print(f\"  Interpretation: \", end='')\n",
    "if dbi_score < 1.0:\n",
    "    print(\"Excellent (well-separated clusters)\")\n",
    "elif dbi_score < 2.0:\n",
    "    print(\"Good (acceptable separation)\")\n",
    "else:\n",
    "    print(\"Poor (high overlap between clusters)\")\n",
    "\n",
    "# 1.3 Calinski-Harabasz Score (variance ratio, higher better)\n",
    "ch_score = calinski_harabasz_score(X_normalized, cluster_labels)\n",
    "print(f\"\\n✓ Calinski-Harabasz Score: {ch_score:.4f}\")\n",
    "print(f\"  Interpretation: Higher = better defined clusters\")\n",
    "\n",
    "# 1.4 Partition Coefficient (FCM-specific, 0 to 1, higher better)\n",
    "u = membership_matrix\n",
    "fpc = np.sum(u ** 2) / X_normalized.shape[0]\n",
    "print(f\"\\n✓ Fuzzy Partition Coefficient (FPC): {fpc:.4f}\")\n",
    "print(f\"  Interpretation: \", end='')\n",
    "if fpc > 0.7:\n",
    "    print(\"High crisp (low fuzziness)\")\n",
    "elif fpc > 0.5:\n",
    "    print(\"Moderate fuzziness (acceptable)\")\n",
    "else:\n",
    "    print(\"High fuzziness (overlapping clusters)\")\n",
    "\n",
    "# 1.5 Partition Entropy (FCM-specific, 0 to log(c), lower better)\n",
    "pe = -np.sum(u * np.log(u + 1e-10)) / X_normalized.shape[0]\n",
    "print(f\"\\n✓ Partition Entropy (PE): {pe:.4f}\")\n",
    "print(f\"  Max entropy (c=2): {np.log(2):.4f}\")\n",
    "print(f\"  Interpretation: \", end='')\n",
    "if pe < np.log(2) * 0.5:\n",
    "    print(\"Low entropy (crisp assignments)\")\n",
    "elif pe < np.log(2) * 0.7:\n",
    "    print(\"Moderate entropy (acceptable)\")\n",
    "else:\n",
    "    print(\"High entropy (fuzzy assignments)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "809bc28c-34c8-4426-b6cc-cb687f4b2599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION 2: CLUSTER COMPACTNESS & SEPARATION\n",
      "================================================================================\n",
      "\n",
      "✓ Cluster 0 WCSS: 775.55\n",
      "  Average distance to center: 8.0392\n",
      "\n",
      "✓ Cluster 1 WCSS: 583.26\n",
      "  Average distance to center: 5.1490\n",
      "\n",
      "✓ Total WCSS: 1358.81\n",
      "\n",
      "✓ Distance between cluster centers: 4.3700\n",
      "  Interpretation: Larger = better separation\n",
      "\n",
      "✓ Dunn Index: 0.2530\n",
      "  Interpretation: Good separation (inter >> intra)\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION 2: CLUSTER COMPACTNESS & SEPARATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION 2: CLUSTER COMPACTNESS & SEPARATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 2.1 Within-cluster sum of squares (WCSS)\n",
    "wcss_total = 0\n",
    "for c_id in [0, 1]:\n",
    "    cluster_points = X_normalized[cluster_labels == c_id]\n",
    "    center = cluster_centers[c_id]\n",
    "    wcss = np.sum((cluster_points - center) ** 2)\n",
    "    wcss_total += wcss\n",
    "    print(f\"\\n✓ Cluster {c_id} WCSS: {wcss:.2f}\")\n",
    "    print(f\"  Average distance to center: {np.sqrt(wcss / len(cluster_points)):.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Total WCSS: {wcss_total:.2f}\")\n",
    "\n",
    "# 2.2 Between-cluster separation (distance between centers)\n",
    "center_distance = np.linalg.norm(cluster_centers[0] - cluster_centers[1])\n",
    "print(f\"\\n✓ Distance between cluster centers: {center_distance:.4f}\")\n",
    "print(f\"  Interpretation: Larger = better separation\")\n",
    "\n",
    "# 2.3 Dunn Index (min inter-cluster / max intra-cluster, higher better)\n",
    "# Simplified version\n",
    "def dunn_index(X, labels, centers):\n",
    "    # Min inter-cluster distance\n",
    "    inter_dist = np.linalg.norm(centers[0] - centers[1])\n",
    "    \n",
    "    # Max intra-cluster distance\n",
    "    max_intra = 0\n",
    "    for c_id in [0, 1]:\n",
    "        cluster_points = X[labels == c_id]\n",
    "        if len(cluster_points) > 1:\n",
    "            dists = cdist(cluster_points, cluster_points)\n",
    "            max_intra = max(max_intra, dists.max())\n",
    "    \n",
    "    return inter_dist / max_intra if max_intra > 0 else 0\n",
    "\n",
    "dunn = dunn_index(X_normalized, cluster_labels, cluster_centers)\n",
    "print(f\"\\n✓ Dunn Index: {dunn:.4f}\")\n",
    "print(f\"  Interpretation: \", end='')\n",
    "if dunn > 0.1:\n",
    "    print(\"Good separation (inter >> intra)\")\n",
    "else:\n",
    "    print(\"Moderate separation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d33bdd1-e0ac-4311-93a8-7c2051f0ce2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION 3: FUZZY MEMBERSHIP QUALITY\n",
      "================================================================================\n",
      "\n",
      "✓ Assignment quality:\n",
      "  Crisp (membership > 0.8): 2/34 (5.9%)\n",
      "  Fuzzy (membership ≤ 0.8): 32/34 (94.1%)\n",
      "\n",
      "✓ Membership statistics:\n",
      "\n",
      "  Cluster 0 (n=12):\n",
      "    Mean membership: 0.6089\n",
      "    Std membership:  0.0462\n",
      "    Min membership:  0.5251\n",
      "    Max membership:  0.6910\n",
      "\n",
      "  Cluster 1 (n=22):\n",
      "    Mean membership: 0.6720\n",
      "    Std membership:  0.0988\n",
      "    Min membership:  0.5062\n",
      "    Max membership:  0.8281\n",
      "\n",
      "✓ Top 5 most ambiguous provinces (closest to 50-50):\n",
      "  1. DKI Jakarta              : C0=0.494, C1=0.506 → Assigned to C1\n",
      "  2. Sulawesi Tengah          : C0=0.493, C1=0.507 → Assigned to C1\n",
      "  3. Riau                     : C0=0.483, C1=0.517 → Assigned to C1\n",
      "  4. Sulawesi Utara           : C0=0.478, C1=0.522 → Assigned to C1\n",
      "  5. Kalimantan Selatan       : C0=0.525, C1=0.475 → Assigned to C0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION 3: MEMBERSHIP QUALITY ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION 3: FUZZY MEMBERSHIP QUALITY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 3.1 Crisp vs Fuzzy assignment ratio\n",
    "max_memberships = membership_matrix.max(axis=0)\n",
    "crisp_mask = max_memberships > 0.8\n",
    "crisp_count = crisp_mask.sum()\n",
    "fuzzy_count = (~crisp_mask).sum()\n",
    "\n",
    "print(f\"\\n✓ Assignment quality:\")\n",
    "print(f\"  Crisp (membership > 0.8): {crisp_count}/{len(provinces)} ({crisp_count/len(provinces)*100:.1f}%)\")\n",
    "print(f\"  Fuzzy (membership ≤ 0.8): {fuzzy_count}/{len(provinces)} ({fuzzy_count/len(provinces)*100:.1f}%)\")\n",
    "\n",
    "# 3.2 Membership statistics per cluster\n",
    "print(f\"\\n✓ Membership statistics:\")\n",
    "for c_id in [0, 1]:\n",
    "    mem = membership_matrix[c_id, :]\n",
    "    assigned_count = (cluster_labels == c_id).sum()\n",
    "    \n",
    "    print(f\"\\n  Cluster {c_id} (n={assigned_count}):\")\n",
    "    print(f\"    Mean membership: {mem[cluster_labels == c_id].mean():.4f}\")\n",
    "    print(f\"    Std membership:  {mem[cluster_labels == c_id].std():.4f}\")\n",
    "    print(f\"    Min membership:  {mem[cluster_labels == c_id].min():.4f}\")\n",
    "    print(f\"    Max membership:  {mem[cluster_labels == c_id].max():.4f}\")\n",
    "\n",
    "# 3.3 Ambiguous cases (closest to 0.5-0.5)\n",
    "ambiguity_scores = np.abs(membership_matrix[0, :] - 0.5)\n",
    "most_ambiguous_idx = np.argsort(ambiguity_scores)[:5]\n",
    "\n",
    "print(f\"\\n✓ Top 5 most ambiguous provinces (closest to 50-50):\")\n",
    "for rank, idx in enumerate(most_ambiguous_idx, 1):\n",
    "    prov = provinces[idx]\n",
    "    mem_c0 = membership_matrix[0, idx]\n",
    "    mem_c1 = membership_matrix[1, idx]\n",
    "    assigned = cluster_labels[idx]\n",
    "    print(f\"  {rank}. {prov:25s}: C0={mem_c0:.3f}, C1={mem_c1:.3f} → Assigned to C{assigned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df4d6d3-9d0d-42aa-b30d-2cb31a2a6c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION 4: CLUSTER STABILITY (BOOTSTRAP)\n",
      "================================================================================\n",
      "\n",
      "Running bootstrap validation (n=50 iterations)...\n",
      "  Progress: 10/50 iterations completed\n",
      "  Progress: 20/50 iterations completed\n",
      "  Progress: 30/50 iterations completed\n",
      "  Progress: 40/50 iterations completed\n",
      "  Progress: 50/50 iterations completed\n",
      "\n",
      "✓ Stability Assessment:\n",
      "  Mean ARI: 0.6647 ± 0.2314\n",
      "  Interpretation: Moderately stable (acceptable)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION 4: CLUSTER STABILITY (BOOTSTRAP)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nRunning bootstrap validation (n=50 iterations)...\")\n",
    "\n",
    "import skfuzzy as fuzz\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "stability_scores = []\n",
    "n_bootstrap = 50\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    # Resample with replacement\n",
    "    indices = resample(np.arange(len(X_normalized)), random_state=i)\n",
    "    X_boot = X_normalized[indices]\n",
    "    \n",
    "    try:\n",
    "        # Run FCM on bootstrapped data\n",
    "        cntr_boot, u_boot, _, _, _, _, _ = fuzz.cluster.cmeans(\n",
    "            X_boot.T,\n",
    "            c=2,\n",
    "            m=2.0,\n",
    "            error=0.005,\n",
    "            maxiter=150,\n",
    "            init=None,\n",
    "            seed=42+i\n",
    "        )\n",
    "        \n",
    "        # Get cluster labels\n",
    "        labels_boot = np.argmax(u_boot, axis=0)\n",
    "        \n",
    "        # Compare with original (for overlapping samples)\n",
    "        ari = adjusted_rand_score(cluster_labels[indices], labels_boot)\n",
    "        stability_scores.append(ari)\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f\"  Progress: {i+1}/50 iterations completed\")\n",
    "\n",
    "ari_mean = np.mean(stability_scores)\n",
    "ari_std = np.std(stability_scores)\n",
    "\n",
    "print(f\"\\n✓ Stability Assessment:\")\n",
    "print(f\"  Mean ARI: {ari_mean:.4f} ± {ari_std:.4f}\")\n",
    "print(f\"  Interpretation: \", end='')\n",
    "if ari_mean > 0.8:\n",
    "    print(\"Highly stable (robust clusters)\")\n",
    "elif ari_mean > 0.6:\n",
    "    print(\"Moderately stable (acceptable)\")\n",
    "else:\n",
    "    print(\"Low stability (sensitive to sampling)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b412d8d-97c6-4744-be67-6ca301d68345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION 5: CONVERGENCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Testing convergence with 10 different random initializations...\n",
      "\n",
      "✓ Convergence statistics (n=10 runs):\n",
      "  Mean iterations: 21.5 ± 6.5\n",
      "  Min iterations:  10\n",
      "  Max iterations:  33\n",
      "\n",
      "✓ FPC consistency:\n",
      "  Mean FPC: 0.5607 ± 0.0001\n",
      "  Interpretation: Highly consistent (stable solution)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION 5: CONVERGENCE ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION 5: CONVERGENCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run FCM multiple times with different initializations\n",
    "convergence_iters = []\n",
    "convergence_fpc = []\n",
    "\n",
    "print(\"\\nTesting convergence with 10 different random initializations...\")\n",
    "\n",
    "for seed in range(10):\n",
    "    try:\n",
    "        cntr_test, u_test, _, _, _, p_test, fpc_test = fuzz.cluster.cmeans(\n",
    "            X_normalized.T,\n",
    "            c=2,\n",
    "            m=2.0,\n",
    "            error=0.005,\n",
    "            maxiter=150,\n",
    "            init=None,\n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "        convergence_iters.append(p_test)\n",
    "        convergence_fpc.append(fpc_test)\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✓ Convergence statistics (n={len(convergence_iters)} runs):\")\n",
    "print(f\"  Mean iterations: {np.mean(convergence_iters):.1f} ± {np.std(convergence_iters):.1f}\")\n",
    "print(f\"  Min iterations:  {np.min(convergence_iters)}\")\n",
    "print(f\"  Max iterations:  {np.max(convergence_iters)}\")\n",
    "print(f\"\\n✓ FPC consistency:\")\n",
    "print(f\"  Mean FPC: {np.mean(convergence_fpc):.4f} ± {np.std(convergence_fpc):.4f}\")\n",
    "print(f\"  Interpretation: \", end='')\n",
    "if np.std(convergence_fpc) < 0.01:\n",
    "    print(\"Highly consistent (stable solution)\")\n",
    "else:\n",
    "    print(\"Some variability (sensitive to initialization)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd639eea-2c01-4197-bbfb-9276b6f3a697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION 6: FEATURE IMPORTANCE FOR CLUSTERING\n",
      "================================================================================\n",
      "\n",
      "Feature type contribution to cluster separation (T-test):\n",
      "----------------------------------------------------------------------\n",
      "Mean        : t=  6.730, p=0.0000 ***, Cohen's d= 2.336\n",
      "CV          : t= -9.255, p=0.0000 ***, Cohen's d=-3.366\n",
      "Trend       : t= -2.372, p=0.0239 *, Cohen's d=-0.869\n",
      "Autocorr    : t= -4.342, p=0.0001 ***, Cohen's d=-1.386\n",
      "Skewness    : t= -0.470, p=0.6413 ns, Cohen's d=-0.168\n",
      "\n",
      "Feature importance ranking (by effect size |Cohen's d|):\n",
      "  CV          : |d|=3.366\n",
      "  Mean        : |d|=2.336\n",
      "  Autocorr    : |d|=1.386\n",
      "  Trend       : |d|=0.869\n",
      "  Skewness    : |d|=0.168\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION 6: FEATURE IMPORTANCE FOR CLUSTERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Feature groups\n",
    "feature_groups = {\n",
    "    'Mean': [c for c in df_features.columns if c.startswith('Mean_')],\n",
    "    'CV': [c for c in df_features.columns if c.startswith('CV_')],\n",
    "    'Trend': [c for c in df_features.columns if c.startswith('Trend_')],\n",
    "    'Autocorr': [c for c in df_features.columns if c.startswith('Autocorr_')],\n",
    "    'Skewness': [c for c in df_features.columns if c.startswith('Skewness_')]\n",
    "}\n",
    "\n",
    "print(\"\\nFeature type contribution to cluster separation (T-test):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "feature_importance = []\n",
    "for ftype, cols in feature_groups.items():\n",
    "    # Average features per province\n",
    "    c0_data = df_features[cluster_labels == 0][cols].mean(axis=1).values\n",
    "    c1_data = df_features[cluster_labels == 1][cols].mean(axis=1).values\n",
    "    \n",
    "    # T-test\n",
    "    t_stat, p_val = stats.ttest_ind(c0_data, c1_data)\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt((np.var(c0_data) + np.var(c1_data)) / 2)\n",
    "    cohens_d = (c0_data.mean() - c1_data.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    significance = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    \n",
    "    feature_importance.append({\n",
    "        'Feature_Type': ftype,\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_val,\n",
    "        'cohens_d': cohens_d,\n",
    "        'significance': significance\n",
    "    })\n",
    "    \n",
    "    print(f\"{ftype:12s}: t={t_stat:7.3f}, p={p_val:.4f} {significance}, Cohen's d={cohens_d:6.3f}\")\n",
    "\n",
    "df_importance = pd.DataFrame(feature_importance).sort_values('cohens_d', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nFeature importance ranking (by effect size |Cohen's d|):\")\n",
    "for i, row in df_importance.iterrows():\n",
    "    print(f\"  {row['Feature_Type']:12s}: |d|={abs(row['cohens_d']):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d135a7-8e54-4ab8-95a6-50eb5953d031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION 7: SAVING EVALUATION REPORT\n",
      "================================================================================\n",
      "\n",
      "✓ Saved: model_evaluation_summary.csv\n",
      "✓ Saved: feature_importance.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION 7: SAVING EVALUATION REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "eval_summary = {\n",
    "    'Metric': [\n",
    "        'Silhouette Score',\n",
    "        'Davies-Bouldin Index',\n",
    "        'Calinski-Harabasz Score',\n",
    "        'Fuzzy Partition Coefficient',\n",
    "        'Partition Entropy',\n",
    "        'WCSS Total',\n",
    "        'Center Distance',\n",
    "        'Dunn Index',\n",
    "        'Crisp Assignments (%)',\n",
    "        'Fuzzy Assignments (%)',\n",
    "        'Bootstrap Stability (ARI)',\n",
    "        'Convergence Iterations (mean)',\n",
    "        'FPC Consistency (std)'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f'{sil_score:.4f}',\n",
    "        f'{dbi_score:.4f}',\n",
    "        f'{ch_score:.2f}',\n",
    "        f'{fpc:.4f}',\n",
    "        f'{pe:.4f}',\n",
    "        f'{wcss_total:.2f}',\n",
    "        f'{center_distance:.4f}',\n",
    "        f'{dunn:.4f}',\n",
    "        f'{crisp_count/len(provinces)*100:.1f}%',\n",
    "        f'{fuzzy_count/len(provinces)*100:.1f}%',\n",
    "        f'{ari_mean:.4f} ± {ari_std:.4f}',\n",
    "        f'{np.mean(convergence_iters):.1f} ± {np.std(convergence_iters):.1f}',\n",
    "        f'{np.std(convergence_fpc):.4f}'\n",
    "    ],\n",
    "    'Interpretation': [\n",
    "        'Good' if sil_score > 0.25 else 'Weak',\n",
    "        'Good' if dbi_score < 2.0 else 'Poor',\n",
    "        'Higher is better',\n",
    "        'Moderate' if fpc > 0.5 else 'High fuzziness',\n",
    "        'Low' if pe < np.log(2)*0.5 else 'Moderate',\n",
    "        'Sum of within-cluster variance',\n",
    "        'Inter-cluster separation',\n",
    "        'Good' if dunn > 0.1 else 'Moderate',\n",
    "        'High crisp ratio' if crisp_count/len(provinces) > 0.7 else 'Moderate',\n",
    "        'Ambiguous assignments',\n",
    "        'Highly stable' if ari_mean > 0.8 else 'Moderate',\n",
    "        'Fast convergence' if np.mean(convergence_iters) < 50 else 'Slow',\n",
    "        'Consistent' if np.std(convergence_fpc) < 0.01 else 'Variable'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_eval_summary = pd.DataFrame(eval_summary)\n",
    "df_eval_summary.to_csv(OUTPUT_DIR + 'model_evaluation_summary.csv', index=False)\n",
    "print(f\"\\n✓ Saved: model_evaluation_summary.csv\")\n",
    "\n",
    "# Save feature importance\n",
    "df_importance.to_csv(OUTPUT_DIR + 'feature_importance.csv', index=False)\n",
    "print(f\"✓ Saved: feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23ba81-40f9-400f-bd69-a2492e3ba01d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
